list("test")
check <- list("test")
append(check, "test2")
job_details <- list()
for (i in 1:5) {
json_return <- execute_scrape_and_prompt(adzuna_api_returns_14jul23$website[i])
job_details <- append(job_details, fromJSON(raw_return))
Sys.sleep(20) ## Given the time to webscrape and call GPT this should be redundant... but I do not feel like getting blocked from either.
}
job_details <- list()
for (i in 1:5) {
json_return <- execute_scrape_and_prompt(adzuna_api_returns_14jul23$website[i])
job_details <- append(job_details, fromJSON(json_return))
Sys.sleep(20) ## Given the time to webscrape and call GPT this should be redundant... but I do not feel like getting blocked from either.
}
job_details
adzuna_api_returns_14jul23$website[1]
job_details <- list()
for (i in 1:nrow(adzuna_api_returns_14jul23)) {
json_return <- execute_scrape_and_prompt(adzuna_api_returns_14jul23$website[i])
job_details <- append(job_details, fromJSON(json_return))
Sys.sleep(20) ## Given the time to webscrape and call GPT this should be redundant... but lets be polite.
}
adzuna_api_returns_14jul23$website[i]
json_return
log("a")
try(log("a"))
try(log("a"))
library(httr)
library(tidyverse)
library(jsonlite)
library(rvest)
library(reticulate)
for (i in 1:10) {
log("a")
}
for (i in 1:10) {
try(log("a"))
}
for (i in 1:10) {
try(
log("a"),
1+1
)
}
for (i in 1:10) {
try(
log("a"),
1
)
}
for (i in 1:10) {
try(
log("a")
)
}
for (i in 1:10) {
try({
log("a")
)
for (i in 1:10) {
try({
log("a")
1+1
})
}
for (i in 1:10) {
try({
log("a", silent = TRUE)
1+i
})
}
for (i in 1:10) {
try({
log("a")
1+i
}, silent = TRUE)
}
##This chunk is concerned with getting the API function working
## Write a function to get any given page. After that I can just iterate while there are still jobs to find :)
get_adzuna_api <- function(api_id, api_key, page,
key_words = c("data", "analyst"), country = "ch") { #API ID and key are variables, so that they can remain secret
key_words <- stringr::str_c(key_words[1], "%20", key_words[2]) #Can only take two key words. All needed for most data professions
api_call_string <- stringr::str_c("http://api.adzuna.com/v1/api/jobs/",
country, "/search/", page, "?app_id=",
api_id, "&app_key=", api_key,
"&results_per_page=20&what=", key_words,
"&max_days_old=730&salary_include_unknown=1&content-type=application/json")
api_return <- GET(api_call_string)
api_return_text <- content(api_return, "text")
api_return_df <- jsonlite::fromJSON(api_return_text)
api_return_df$results
}
## This code block is about removing unwanted columns from the APi response and renaming other ones.
## Functionalizing for easier integration into the automated pipeline downstream
clean_adzuna_response <- function(adzuna_response) {
response_cleaned <- data.frame(
date_posted = adzuna_response$created,
job_title = adzuna_response$title,
company = adzuna_response$company$display_name,
location = adzuna_response$location$display_name,
website = adzuna_response$redirect_url
) %>%
filter(str_detect(website, "details")) #Adzuna links quite a few instances of external job boards
response_cleaned
}
### At first, this will be a mess of re-learning how to web scrape :P
parse_adzuna_data <- function (adzuna_http){
adzuna_html_response <- read_html(adzuna_http) #Get the website
adzuna_body <- adzuna_html_response %>% #Extract the body of the job ad
html_nodes(".adp-body") %>%
html_text()
adzuna_body
}
reticulate::repl_python()
execute_scrape_and_prompt <- function (adzuna_address){
adzuna_raw_return <- parse_adzuna_data(adzuna_address)
py$summarize_adzuna(adzuna_raw_return) ## The py$ calls from the python side of the environment
}
adzuna_api_returns_14jul23 <- readRDS("adzuna_api_returns_14jul23.rds")
job_details <- list()
for (i in 1:nrow(adzuna_api_returns_14jul23)) {
try({   ###The try function is here in case any web scrape fails
json_return <- execute_scrape_and_prompt(
adzuna_api_returns_14jul23$website[i])
job_details <- append(job_details, fromJSON(json_return))
}, silent = TRUE
)
Sys.sleep(30) ## Given the time to webscrape and call GPT this should be redundant... but lets be polite.
}
View(job_details)
write_rds(job_details, "job_details_14jul23.rds")
library(httr)
library(tidyverse)
library(jsonlite)
library(rvest)
library(reticulate)
json_results <- readRDS("job_details_14jul23.rds")
job_details <- as_tibble(json_results)
job_details <- fromJSON(json_results)
job_details <- toJSON(json_results)
json_results[1]
json_results[1][1]
json_results[[1]]
json_results[[1]][2]
json_results[[1]][[2]]
json_results[["skills"]]
json_results["skills"]
unlist(json_results)
flatten(json_results)
map_chr(l, "technical skills")
map_chr(json_results, "technical skills")
map_chr(json_results, "skills")
json_results[["skills"]]
json_results[["skills"]][2]
json_results[["skills"]][c("technical skills")]
json_results[["skills"]][c("technical skills", "soft skills")]
json_results[["technical skills"]][c("technical skills")]
json_results[["technical skills"]]
map_dfr(json_results, extract, "technical skills")
map_dfr(json_results, extract, "skills")
##This chunk is concerned with getting the API function working
## Write a function to get any given page. After that I can just iterate while there are still jobs to find :)
get_adzuna_api <- function(api_id, api_key, page,
key_words = c("data", "analyst"), country = "ch") { #API ID and key are variables, so that they can remain secret
key_words <- stringr::str_c(key_words[1], "%20", key_words[2]) #Can only take two key words. All needed for most data professions
api_call_string <- stringr::str_c("http://api.adzuna.com/v1/api/jobs/",
country, "/search/", page, "?app_id=",
api_id, "&app_key=", api_key,
"&results_per_page=20&what=", key_words,
"&max_days_old=730&salary_include_unknown=1&content-type=application/json")
api_return <- GET(api_call_string)
api_return_text <- content(api_return, "text")
api_return_df <- jsonlite::fromJSON(api_return_text)
api_return_df$results
}
## This code block is about removing unwanted columns from the APi response and renaming other ones.
## Functionalizing for easier integration into the automated pipeline downstream
clean_adzuna_response <- function(adzuna_response) {
response_cleaned <- data.frame(
date_posted = adzuna_response$created,
job_title = adzuna_response$title,
company = adzuna_response$company$display_name,
location = adzuna_response$location$display_name,
website = adzuna_response$redirect_url
) %>%
filter(str_detect(website, "details")) #Adzuna links quite a few instances of external job boards
response_cleaned
}
### At first, this will be a mess of re-learning how to web scrape :P
parse_adzuna_data <- function (adzuna_http){
adzuna_html_response <- read_html(adzuna_http) #Get the website
adzuna_body <- adzuna_html_response %>% #Extract the body of the job ad
html_nodes(".adp-body") %>%
html_text()
adzuna_body
}
reticulate::repl_python()
execute_scrape_and_prompt <- function (adzuna_address){
adzuna_raw_return <- parse_adzuna_data(adzuna_address)
py$summarize_adzuna(adzuna_raw_return) ## The py$ calls from the python side of the environment
}
adzuna_api_returns_14jul23 <- readRDS("adzuna_api_returns_14jul23.rds")
execute_scrape_and_prompt(adzuna_api_returns_14jul23[1])
execute_scrape_and_prompt(adzuna_api_returns_14jul23$website[1])
test6 <- execute_scrape_and_prompt(adzuna_api_returns_14jul23$website[1])
as_tibble(test6)
fromJSON(test6)
fromJSON(test6) %>%
as_tibble
fromJSON(test6) %>%
as_tibble %>%
unnest()
fromJSON(test6) %>%
select("technical skills")
fromJSON(test6)
fromJSON(test6)[[1]]
fromJSON(test6)[[1]][1]
fromJSON(test6)[[1]][2]
fromJSON(test6)[[1]][[2]]
soft_skills <- c()
tech_skills <- c()
qualifications <- c()
for (i in 1:4) {
try({   ###The try function is here in case any web scrape fails
json_return <- execute_scrape_and_prompt(
adzuna_api_returns_14jul23$website[i]
)
soft_skills <- c(soft_skills, fromJSON(json_return)[[1]][[1]])
tech_skills <- c(tech_skills, fromJSON(json_return)[[1]][[2]])
qualifications <- c(qualifications, fromJSON(json_return[[1]][[3]]))
}, silent = TRUE
)
Sys.sleep(30) ## Given the time to webscrape and call GPT this should be redundant... but lets be polite.
}
fromJSON(test6)[[1]][[3]]
test7 <- c()
flatten(test6)
fromJSON(test6) %>% flatten()
fromJSON(test6) %>% as_tibble
fromJSON(test6) %>% as_tibble()
fromJSON(test6) %>% as_tibble() %>%unnest()
fromJSON(test6) %>% as_tibble() %>% unnest() %>% pull(skills)
skills <- c()
for (i in 1:4) {
try({   ###The try function is here in case any web scrape fails
json_return <- execute_scrape_and_prompt(
adzuna_api_returns_14jul23$website[i]
)
skills_from_return <- json_return %>%
fromJSON() %>%
as_tibble() %>%
pull("skills")
skills <- c(skills, skills_from_return)
}, silent = TRUE
)
Sys.sleep(30) ## Given the time to webscrape and call GPT this should be redundant... but lets be polite.
}
fromJSON(test6) %>% as_tibble() %>% unnest() %>% pull(skills)
skills <- c()
for (i in 1:2) {
try({   ###The try function is here in case any web scrape fails
json_return <- execute_scrape_and_prompt(
adzuna_api_returns_14jul23$website[i]
)
skills_from_return <- json_return %>%
fromJSON() %>%
as_tibble(cols = c(skills)) %>%
pull(skills) ##This will flatten the information on tech, soft skills and qualifications. Having said that: 1) reviewing the test data, GPT got confused a lot of time about those; 2) We don't strictly need those.
skills <- c(skills, skills_from_return)
}, silent = TRUE
)
Sys.sleep(30) ## Given the time to webscrape and call GPT this should be redundant... but lets be polite.
}
fromJSON(test6) %>% as_tibble() %>% unnest(cols = c(skills)) %>% pull(skills)
for (i in 1:2) {
try({   ###The try function is here in case any web scrape fails
json_return <- execute_scrape_and_prompt(
adzuna_api_returns_14jul23$website[i]
)
skills_from_return <- json_return %>%
fromJSON() %>%
as_tibble(cols = c(skills)) %>%
unnest() %>%
pull(skills) ##This will flatten the information on tech, soft skills and qualifications. Having said that: 1) reviewing the test data, GPT got confused a lot of time about those; 2) We don't strictly need those.
skills <- c(skills, skills_from_return)
}, silent = TRUE
)
Sys.sleep(30) ## Given the time to webscrape and call GPT this should be redundant... but lets be polite.
}
skills <- c()
for (i in 1:2) {
try({   ###The try function is here in case any web scrape fails
json_return <- execute_scrape_and_prompt(
adzuna_api_returns_14jul23$website[i]
)
skills_from_return <- json_return %>%
fromJSON() %>%
as_tibble() %>%
unnest(cols = c(skills)) %>%
pull(skills) ##This will flatten the information on tech, soft skills and qualifications. Having said that: 1) reviewing the test data, GPT got confused a lot of time about those; 2) We don't strictly need those.
skills <- c(skills, skills_from_return)
}, silent = TRUE
)
Sys.sleep(30) ## Given the time to webscrape and call GPT this should be redundant... but lets be polite.
}
skills
skills <- c()
for (i in 3:4) {
try({   ###The try function is here in case any web scrape fails
json_return <- execute_scrape_and_prompt(
adzuna_api_returns_14jul23$website[i]
)
skills_from_return <- json_return %>%
fromJSON() %>%
as_tibble() %>%
unnest(cols = c(skills)) %>%
pull(skills) ##This will flatten the information on tech, soft skills and qualifications. Having said that: 1) reviewing the test data, GPT got confused a lot of time about those; 2) We don't strictly need those.
skills <- c(skills, skills_from_return)
}, silent = TRUE
)
Sys.sleep(30) ## Given the time to webscrape and call GPT this should be redundant... but lets be polite.
}
library(httr)
library(tidyverse)
library(jsonlite)
library(rvest)
library(reticulate)
##This chunk is concerned with getting the API function working
## Write a function to get any given page. After that I can just iterate while there are still jobs to find :)
get_adzuna_api <- function(api_id, api_key, page,
key_words = c("data", "analyst"), country = "ch") { #API ID and key are variables, so that they can remain secret
key_words <- stringr::str_c(key_words[1], "%20", key_words[2]) #Can only take two key words. All needed for most data professions
api_call_string <- stringr::str_c("http://api.adzuna.com/v1/api/jobs/",
country, "/search/", page, "?app_id=",
api_id, "&app_key=", api_key,
"&results_per_page=20&what=", key_words,
"&max_days_old=730&salary_include_unknown=1&content-type=application/json")
api_return <- GET(api_call_string)
api_return_text <- content(api_return, "text")
api_return_df <- jsonlite::fromJSON(api_return_text)
api_return_df$results
}
## Test the function
test <- get_adzuna_api(
readLines("/home/pete/Documents/adzuna_api.txt")[1],
readLines("/home/pete/Documents/adzuna_api.txt")[2], 10)
test$location$display_name
## This code block is about removing unwanted columns from the APi response and renaming other ones.
## Functionalizing for easier integration into the automated pipeline downstream
clean_adzuna_response <- function(adzuna_response) {
response_cleaned <- data.frame(
date_posted = adzuna_response$created,
job_title = adzuna_response$title,
company = adzuna_response$company$display_name,
location = adzuna_response$location$display_name,
website = adzuna_response$redirect_url
) %>%
filter(str_detect(website, "details")) #Adzuna links quite a few instances of external job boards
response_cleaned
}
### At first, this will be a mess of re-learning how to web scrape :P
parse_adzuna_data <- function (adzuna_http){
adzuna_html_response <- read_html(adzuna_http) #Get the website
adzuna_body <- adzuna_html_response %>% #Extract the body of the job ad
html_nodes(".adp-body") %>%
html_text()
adzuna_body
}
reticulate::repl_python()
library(httr)
library(tidyverse)
library(jsonlite)
library(rvest)
library(reticulate)
## Test the function
test <- get_adzuna_api(
readLines("/home/pete/Documents/adzuna_api.txt")[1],
readLines("/home/pete/Documents/adzuna_api.txt")[2], 10)
##This chunk is concerned with getting the API function working
## Write a function to get any given page. After that I can just iterate while there are still jobs to find :)
get_adzuna_api <- function(api_id, api_key, page,
key_words = c("data", "analyst"), country = "ch") { #API ID and key are variables, so that they can remain secret
key_words <- stringr::str_c(key_words[1], "%20", key_words[2]) #Can only take two key words. All needed for most data professions
api_call_string <- stringr::str_c("http://api.adzuna.com/v1/api/jobs/",
country, "/search/", page, "?app_id=",
api_id, "&app_key=", api_key,
"&results_per_page=20&what=", key_words,
"&max_days_old=730&salary_include_unknown=1&content-type=application/json")
api_return <- GET(api_call_string)
api_return_text <- content(api_return, "text")
api_return_df <- jsonlite::fromJSON(api_return_text)
api_return_df$results
}
## This code block is about removing unwanted columns from the APi response and renaming other ones.
## Functionalizing for easier integration into the automated pipeline downstream
clean_adzuna_response <- function(adzuna_response) {
response_cleaned <- data.frame(
date_posted = adzuna_response$created,
job_title = adzuna_response$title,
company = adzuna_response$company$display_name,
location = adzuna_response$location$display_name,
website = adzuna_response$redirect_url
) %>%
filter(str_detect(website, "details")) #Adzuna links quite a few instances of external job boards
response_cleaned
}
### At first, this will be a mess of re-learning how to web scrape :P
parse_adzuna_data <- function (adzuna_http){
adzuna_html_response <- read_html(adzuna_http) #Get the website
adzuna_body <- adzuna_html_response %>% #Extract the body of the job ad
html_nodes(".adp-body") %>%
html_text()
adzuna_body
}
reticulate::repl_python()
execute_scrape_and_prompt <- function (adzuna_address){
adzuna_raw_return <- parse_adzuna_data(adzuna_address)
py$summarize_adzuna(adzuna_raw_return) ## The py$ calls from the python side of the environment
}
adzuna_api_returns_14jul23 <- readRDS("adzuna_api_returns_14jul23.rds")
skills <- list()
for (i in 1:4) {
try({   ###The try function is here in case any web scrape fails
json_return <- execute_scrape_and_prompt(
adzuna_api_returns_14jul23$website[i]
)
skills_from_return <- json_return %>%
fromJSON() %>%
as_tibble() %>%
unnest(cols = c(skills)) %>%
pull(skills) ##This will flatten the information on tech, soft skills and qualifications. Having said that: 1) reviewing the test data, GPT got confused a lot of time about those; 2) We don't strictly need those.
skills <- append(skills, skills_from_return)
}, silent = TRUE
)
Sys.sleep(30) ## Given the time to webscrape and call GPT this should be redundant... but lets be polite.
}
skills <- c()
for (i in 1:4) {
try({   ###The try function is here in case any web scrape fails
json_return <- execute_scrape_and_prompt(
adzuna_api_returns_14jul23$website[i]
)
skills_from_return <- json_return %>%
fromJSON() %>%
as_tibble() %>%
unnest(cols = c(skills)) %>%
pull(skills) ##This will flatten the information on tech, soft skills and qualifications. Having said that: 1) reviewing the test data, GPT got confused a lot of time about those; 2) We don't strictly need those.
skills <- c(skills, skills_from_return)
}, silent = TRUE
)
Sys.sleep(30) ## Given the time to webscrape and call GPT this should be redundant... but lets be polite.
}
for (i in 1:4) {
try({   ###The try function is here in case any web scrape fails
json_return <- execute_scrape_and_prompt(
adzuna_api_returns_14jul23$website[i]
)
skills_from_return <- json_return %>%
fromJSON() %>%
as_tibble() %>%
unnest(cols = c(skills)) %>%
pull(skills) ##This will flatten the information on tech, soft skills and qualifications. Having said that: 1) reviewing the test data, GPT got confused a lot of time about those; 2) We don't strictly need those.
skills <- append(skills, skills_from_return)
}, silent = TRUE
)
Sys.sleep(30) ## Given the time to webscrape and call GPT this should be redundant... but lets be polite.
}
skills <- list()
for (i in 1:4) {
try({   ###The try function is here in case any web scrape fails
json_return <- execute_scrape_and_prompt(
adzuna_api_returns_14jul23$website[i]
)
skills_from_return <- json_return %>%
fromJSON() %>%
as_tibble() %>%
unnest(cols = c(skills)) %>%
pull(skills) ##This will flatten the information on tech, soft skills and qualifications. Having said that: 1) reviewing the test data, GPT got confused a lot of time about those; 2) We don't strictly need those.
skills <- append(skills, skills_from_return)
}, silent = TRUE
)
Sys.sleep(30) ## Given the time to webscrape and call GPT this should be redundant... but lets be polite.
}
skills <- unlist(skills)
skills_df <- data.frame(skills = skills) %>%
group_by(skills) %>%
summarize(
keyword_count = n()
)
View(skills_df)
skills_df <- data.frame(skills = skills) %>%
group_by(skills) %>%
summarize(
keyword_count = n()
) %>%
arrange(keyword_count)
View(skills_df)
skills_df <- data.frame(skills = skills) %>%
group_by(skills) %>%
summarize(
keyword_count = n()
) %>%
arrange(desc(keyword_count))
View(skills_df)
##Lets get a URL to visit
test2 <- read_csv("test_returns.csv")
test2$website[4]
##Get a return to work with for dev
test3 <- parse_adzuna_data(test2$website[2])
test3
reticulate::repl_python()
