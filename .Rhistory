)
skills_from_return <- json_return %>%
fromJSON() %>%
as_tibble(cols = c(skills)) %>%
unnest() %>%
pull(skills) ##This will flatten the information on tech, soft skills and qualifications. Having said that: 1) reviewing the test data, GPT got confused a lot of time about those; 2) We don't strictly need those.
skills <- c(skills, skills_from_return)
}, silent = TRUE
)
Sys.sleep(30) ## Given the time to webscrape and call GPT this should be redundant... but lets be polite.
}
skills <- c()
for (i in 1:2) {
try({   ###The try function is here in case any web scrape fails
json_return <- execute_scrape_and_prompt(
adzuna_api_returns_14jul23$website[i]
)
skills_from_return <- json_return %>%
fromJSON() %>%
as_tibble() %>%
unnest(cols = c(skills)) %>%
pull(skills) ##This will flatten the information on tech, soft skills and qualifications. Having said that: 1) reviewing the test data, GPT got confused a lot of time about those; 2) We don't strictly need those.
skills <- c(skills, skills_from_return)
}, silent = TRUE
)
Sys.sleep(30) ## Given the time to webscrape and call GPT this should be redundant... but lets be polite.
}
skills
skills <- c()
for (i in 3:4) {
try({   ###The try function is here in case any web scrape fails
json_return <- execute_scrape_and_prompt(
adzuna_api_returns_14jul23$website[i]
)
skills_from_return <- json_return %>%
fromJSON() %>%
as_tibble() %>%
unnest(cols = c(skills)) %>%
pull(skills) ##This will flatten the information on tech, soft skills and qualifications. Having said that: 1) reviewing the test data, GPT got confused a lot of time about those; 2) We don't strictly need those.
skills <- c(skills, skills_from_return)
}, silent = TRUE
)
Sys.sleep(30) ## Given the time to webscrape and call GPT this should be redundant... but lets be polite.
}
library(httr)
library(tidyverse)
library(jsonlite)
library(rvest)
library(reticulate)
##This chunk is concerned with getting the API function working
## Write a function to get any given page. After that I can just iterate while there are still jobs to find :)
get_adzuna_api <- function(api_id, api_key, page,
key_words = c("data", "analyst"), country = "ch") { #API ID and key are variables, so that they can remain secret
key_words <- stringr::str_c(key_words[1], "%20", key_words[2]) #Can only take two key words. All needed for most data professions
api_call_string <- stringr::str_c("http://api.adzuna.com/v1/api/jobs/",
country, "/search/", page, "?app_id=",
api_id, "&app_key=", api_key,
"&results_per_page=20&what=", key_words,
"&max_days_old=730&salary_include_unknown=1&content-type=application/json")
api_return <- GET(api_call_string)
api_return_text <- content(api_return, "text")
api_return_df <- jsonlite::fromJSON(api_return_text)
api_return_df$results
}
## Test the function
test <- get_adzuna_api(
readLines("/home/pete/Documents/adzuna_api.txt")[1],
readLines("/home/pete/Documents/adzuna_api.txt")[2], 10)
test$location$display_name
## This code block is about removing unwanted columns from the APi response and renaming other ones.
## Functionalizing for easier integration into the automated pipeline downstream
clean_adzuna_response <- function(adzuna_response) {
response_cleaned <- data.frame(
date_posted = adzuna_response$created,
job_title = adzuna_response$title,
company = adzuna_response$company$display_name,
location = adzuna_response$location$display_name,
website = adzuna_response$redirect_url
) %>%
filter(str_detect(website, "details")) #Adzuna links quite a few instances of external job boards
response_cleaned
}
### At first, this will be a mess of re-learning how to web scrape :P
parse_adzuna_data <- function (adzuna_http){
adzuna_html_response <- read_html(adzuna_http) #Get the website
adzuna_body <- adzuna_html_response %>% #Extract the body of the job ad
html_nodes(".adp-body") %>%
html_text()
adzuna_body
}
reticulate::repl_python()
library(httr)
library(tidyverse)
library(jsonlite)
library(rvest)
library(reticulate)
## Test the function
test <- get_adzuna_api(
readLines("/home/pete/Documents/adzuna_api.txt")[1],
readLines("/home/pete/Documents/adzuna_api.txt")[2], 10)
##This chunk is concerned with getting the API function working
## Write a function to get any given page. After that I can just iterate while there are still jobs to find :)
get_adzuna_api <- function(api_id, api_key, page,
key_words = c("data", "analyst"), country = "ch") { #API ID and key are variables, so that they can remain secret
key_words <- stringr::str_c(key_words[1], "%20", key_words[2]) #Can only take two key words. All needed for most data professions
api_call_string <- stringr::str_c("http://api.adzuna.com/v1/api/jobs/",
country, "/search/", page, "?app_id=",
api_id, "&app_key=", api_key,
"&results_per_page=20&what=", key_words,
"&max_days_old=730&salary_include_unknown=1&content-type=application/json")
api_return <- GET(api_call_string)
api_return_text <- content(api_return, "text")
api_return_df <- jsonlite::fromJSON(api_return_text)
api_return_df$results
}
## This code block is about removing unwanted columns from the APi response and renaming other ones.
## Functionalizing for easier integration into the automated pipeline downstream
clean_adzuna_response <- function(adzuna_response) {
response_cleaned <- data.frame(
date_posted = adzuna_response$created,
job_title = adzuna_response$title,
company = adzuna_response$company$display_name,
location = adzuna_response$location$display_name,
website = adzuna_response$redirect_url
) %>%
filter(str_detect(website, "details")) #Adzuna links quite a few instances of external job boards
response_cleaned
}
### At first, this will be a mess of re-learning how to web scrape :P
parse_adzuna_data <- function (adzuna_http){
adzuna_html_response <- read_html(adzuna_http) #Get the website
adzuna_body <- adzuna_html_response %>% #Extract the body of the job ad
html_nodes(".adp-body") %>%
html_text()
adzuna_body
}
reticulate::repl_python()
execute_scrape_and_prompt <- function (adzuna_address){
adzuna_raw_return <- parse_adzuna_data(adzuna_address)
py$summarize_adzuna(adzuna_raw_return) ## The py$ calls from the python side of the environment
}
adzuna_api_returns_14jul23 <- readRDS("adzuna_api_returns_14jul23.rds")
skills <- list()
for (i in 1:4) {
try({   ###The try function is here in case any web scrape fails
json_return <- execute_scrape_and_prompt(
adzuna_api_returns_14jul23$website[i]
)
skills_from_return <- json_return %>%
fromJSON() %>%
as_tibble() %>%
unnest(cols = c(skills)) %>%
pull(skills) ##This will flatten the information on tech, soft skills and qualifications. Having said that: 1) reviewing the test data, GPT got confused a lot of time about those; 2) We don't strictly need those.
skills <- append(skills, skills_from_return)
}, silent = TRUE
)
Sys.sleep(30) ## Given the time to webscrape and call GPT this should be redundant... but lets be polite.
}
skills <- c()
for (i in 1:4) {
try({   ###The try function is here in case any web scrape fails
json_return <- execute_scrape_and_prompt(
adzuna_api_returns_14jul23$website[i]
)
skills_from_return <- json_return %>%
fromJSON() %>%
as_tibble() %>%
unnest(cols = c(skills)) %>%
pull(skills) ##This will flatten the information on tech, soft skills and qualifications. Having said that: 1) reviewing the test data, GPT got confused a lot of time about those; 2) We don't strictly need those.
skills <- c(skills, skills_from_return)
}, silent = TRUE
)
Sys.sleep(30) ## Given the time to webscrape and call GPT this should be redundant... but lets be polite.
}
for (i in 1:4) {
try({   ###The try function is here in case any web scrape fails
json_return <- execute_scrape_and_prompt(
adzuna_api_returns_14jul23$website[i]
)
skills_from_return <- json_return %>%
fromJSON() %>%
as_tibble() %>%
unnest(cols = c(skills)) %>%
pull(skills) ##This will flatten the information on tech, soft skills and qualifications. Having said that: 1) reviewing the test data, GPT got confused a lot of time about those; 2) We don't strictly need those.
skills <- append(skills, skills_from_return)
}, silent = TRUE
)
Sys.sleep(30) ## Given the time to webscrape and call GPT this should be redundant... but lets be polite.
}
skills <- list()
for (i in 1:4) {
try({   ###The try function is here in case any web scrape fails
json_return <- execute_scrape_and_prompt(
adzuna_api_returns_14jul23$website[i]
)
skills_from_return <- json_return %>%
fromJSON() %>%
as_tibble() %>%
unnest(cols = c(skills)) %>%
pull(skills) ##This will flatten the information on tech, soft skills and qualifications. Having said that: 1) reviewing the test data, GPT got confused a lot of time about those; 2) We don't strictly need those.
skills <- append(skills, skills_from_return)
}, silent = TRUE
)
Sys.sleep(30) ## Given the time to webscrape and call GPT this should be redundant... but lets be polite.
}
skills <- unlist(skills)
skills_df <- data.frame(skills = skills) %>%
group_by(skills) %>%
summarize(
keyword_count = n()
)
View(skills_df)
skills_df <- data.frame(skills = skills) %>%
group_by(skills) %>%
summarize(
keyword_count = n()
) %>%
arrange(keyword_count)
View(skills_df)
skills_df <- data.frame(skills = skills) %>%
group_by(skills) %>%
summarize(
keyword_count = n()
) %>%
arrange(desc(keyword_count))
View(skills_df)
##Lets get a URL to visit
test2 <- read_csv("test_returns.csv")
test2$website[4]
##Get a return to work with for dev
test3 <- parse_adzuna_data(test2$website[2])
test3
reticulate::repl_python()
execute_scrape_and_prompt <- function (adzuna_address){
adzuna_raw_return <- parse_adzuna_data(adzuna_address)
py$summarize_adzuna(adzuna_raw_return) ## The py$ calls from the python side of the environment
}
skills <- list()
for (i in 1:4) {
try({   ###The try function is here in case any web scrape fails
json_return <- execute_scrape_and_prompt(
adzuna_api_returns_14jul23$website[i]
)
skills_from_return <- json_return %>%
fromJSON() %>%
as_tibble() %>%
unnest(cols = c(skills)) %>%
pull(skills) ##This will flatten the information on tech, soft skills and qualifications. Having said that: 1) reviewing the test data, GPT got confused a lot of time about those; 2) We don't strictly need those.
skills <- append(skills, skills_from_return)
}, silent = TRUE
)
Sys.sleep(30) ## Given the time to webscrape and call GPT this should be redundant... but lets be polite.
}
skills <- unlist(skills)
skills_df <- data.frame(skills = skills) %>%
group_by(skills) %>%
summarize(
keyword_count = n()
) %>%
arrange(desc(keyword_count))
View(skills_df)
for (i in 1:4) {
try({   ###The try function is here in case any web scrape fails
json_return <- execute_scrape_and_prompt(
adzuna_api_returns_14jul23$website[i]
)
skills_from_return <- json_return %>%
fromJSON() %>%
as_tibble() %>%
unnest(cols = c(skills)) %>%
pull(skills) ##This will flatten the information on tech, soft skills and qualifications. Having said that: 1) reviewing the test data, GPT got confused a lot of time about those; 2) We don't strictly need those.
}, silent = TRUE
)
skills <- c(skills, skills_from_return)
json_return <- list() ### Keep the return empty in case a webscrape or GPT call fails - we don't want to double record same entry
Sys.sleep(30) ## Given the time to webscrape and call GPT this should be redundant... but lets be polite.
}
skills <- c()
for (i in 1:4) {
try({   ###The try function is here in case any web scrape fails
json_return <- execute_scrape_and_prompt(
adzuna_api_returns_14jul23$website[i]
)
skills_from_return <- json_return %>%
fromJSON() %>%
as_tibble() %>%
unnest(cols = c(skills)) %>%
pull(skills) ##This will flatten the information on tech, soft skills and qualifications. Having said that: 1) reviewing the test data, GPT got confused a lot of time about those; 2) We don't strictly need those.
}, silent = TRUE
)
skills <- c(skills, skills_from_return)
json_return <- list() ### Keep the return empty in case a webscrape or GPT call fails - we don't want to double record same entry
Sys.sleep(30) ## Given the time to webscrape and call GPT this should be redundant... but lets be polite.
}
skills_df <- data.frame(skills = skills) %>%
group_by(skills) %>%
summarize(
keyword_count = n()
) %>%
arrange(desc(keyword_count))
View(skills_df)
skills
for (i in 1:4) {
try({   ###The try function is here in case any web scrape fails
json_return <- execute_scrape_and_prompt(
adzuna_api_returns_14jul23$website[i]
)
skills_from_return <- json_return %>%
fromJSON() %>%
as_tibble() %>%
unnest(cols = c(skills)) %>%
pull(skills) ##This will flatten the information on tech, soft skills and qualifications. Having said that: 1) reviewing the test data, GPT got confused a lot of time about those; 2) We don't strictly need those.
}, silent = TRUE
)
skills <- c(skills, skills_from_return)
skills_from_return <- c() ### Keep the return empty in case a webscrape or GPT call fails - we don't want to double record same entry
Sys.sleep(30) ## Given the time to webscrape and call GPT this should be redundant... but lets be polite.
}
skills <- c()
for (i in 1:4) {
try({   ###The try function is here in case any web scrape fails
json_return <- execute_scrape_and_prompt(
adzuna_api_returns_14jul23$website[i]
)
skills_from_return <- json_return %>%
fromJSON() %>%
as_tibble() %>%
unnest(cols = c(skills)) %>%
pull(skills) ##This will flatten the information on tech, soft skills and qualifications. Having said that: 1) reviewing the test data, GPT got confused a lot of time about those; 2) We don't strictly need those.
}, silent = TRUE
)
skills <- c(skills, skills_from_return)
skills_from_return <- c() ### Keep the return empty in case a webscrape or GPT call fails - we don't want to double record same entry
Sys.sleep(30) ## Given the time to webscrape and call GPT this should be redundant... but lets be polite.
}
json_return <- list() ### Keep the return empty in case a webscrape or GPT call fails - we don't want to double record same entry
skills_from_return <- json_return %>%
as_tibble() %>%
unnest(cols = c(skills)) %>%
pull(skills) ##This will flatten the information on tech, soft skills and qualifications. Having said that: 1) reviewing the test data, GPT got confused a lot of time about those; 2) We don't strictly need those.
try({   ###The try function is here in case any web scrape fails
json_return <- execute_scrape_and_prompt(
adzuna_api_returns_14jul23$website[5]
) %>%
fromJSON()
}, silent = TRUE
)
skills_from_return <- json_return %>%
as_tibble() %>%
unnest(cols = c(skills)) %>%
pull(skills) ##This will flatten the information on tech, soft skills and qualifications. Having said that: 1) reviewing the test data, GPT got confused a lot of time about those; 2) We don't strictly need those.
View(json_return)
skills_from_return <- json_return %>%
as_tibble() %>%
unnest() %>%
pull(skills) ##This will flatten the information on tech, soft skills and qualifications. Having said that: 1) reviewing the test data, GPT got confused a lot of time about those; 2) We don't strictly need those.
skills_from_return <- json_return %>%
unlist()
skills_from_return
skills <- c()
skills <- c(skills, skills_from_return)
skills <- c(skills, skills_from_return)
json_return <- TRUE ### Keep the return empty in case a webscrape or GPT call fails - we don't want to double record same entry
try({   ###The try function is here in case any web scrape fails
json_return <- execute_scrape_and_prompt(
adzuna_api_returns_14jul23$website["potato"]
) %>%
fromJSON()
}, silent = TRUE
)
json_return <- execute_scrape_and_prompt(
adzuna_api_returns_14jul23$website["potato"]
) %>%
fromJSON()
if (json_return) {0} else {1}
try({   ###The try function is here in case any web scrape fails
json_return <- execute_scrape_and_prompt(
adzuna_api_returns_14jul23$website[5]
) %>%
fromJSON()
}, silent = TRUE
)
if (json_return) {0} else {1}
if (json_return == TRUE) {0} else {1}
if (is.logical(json_return)) {0} else {1}
skills_from_return <- c()
if (is.logical(json_return)) {
skills_from_return <- c()
} else {
skills_from_return <- unlist(json_return)
}
json_return <- TRUE ### Keep the return empty in case a webscrape or GPT call fails - we don't want to double record same entry
if (is.logical(json_return)) {
skills_from_return <- c()
} else {
skills_from_return <- unlist(json_return)
}
skills <- c(skills, skills_from_return)
skills <- c()
skills <- c()
for (i in 5:6) {
try({   ###The try function is here in case any web scrape fails
json_return <- execute_scrape_and_prompt(
adzuna_api_returns_14jul23$website[5]
) %>%
fromJSON()
}, silent = TRUE
)
if (is.logical(json_return)) {  ##If JSON return is logical, it means that something above failed, so we want to have an empty entry
skills_from_return <- c()
} else {
skills_from_return <- unlist(json_return)
}
skills <- c(skills, skills_from_return)
json_return <- TRUE ### Keep the return empty in case a webscrape or GPT call fails - we don't want to double record same entry
Sys.sleep(30) ## Given the time to webscrape and call GPT this should be redundant... but lets be polite.
}
json_return <- execute_scrape_and_prompt(
adzuna_api_returns_14jul23$website[6]
) %>%
fromJSON()
skills <- c()
for (i in 5:6) {
try({   ###The try function is here in case any web scrape fails
json_return <- execute_scrape_and_prompt(
adzuna_api_returns_14jul23$website[i]
) %>%
fromJSON()
}, silent = TRUE
)
if (is.logical(json_return)) {  ##If JSON return is logical, it means that something above failed, so we want to have an empty entry
skills_from_return <- c()
} else {
skills_from_return <- unlist(json_return)
}
skills <- c(skills, skills_from_return)
json_return <- TRUE ### Keep the return empty in case a webscrape or GPT call fails - we don't want to double record same entry
Sys.sleep(30) ## Given the time to webscrape and call GPT this should be redundant... but lets be polite.
}
skills <- c()
for (i in 1:6) {
try({   ###The try function is here in case any web scrape fails
json_return <- execute_scrape_and_prompt(
adzuna_api_returns_14jul23$website[i]
) %>%
fromJSON()
}, silent = TRUE
)
if (is.logical(json_return)) {  ##If JSON return is logical, it means that something above failed, so we want to have an empty entry
skills_from_return <- c()
} else {
skills_from_return <- unlist(json_return)
}
skills <- c(skills, skills_from_return)
json_return <- TRUE ### Keep the return empty in case a webscrape or GPT call fails - we don't want to double record same entry
Sys.sleep(30) ## Given the time to webscrape and call GPT this should be redundant... but lets be polite.
}
skills_df <- data.frame(skills = skills) %>%
group_by(skills) %>%
summarize(
keyword_count = n()
) %>%
arrange(desc(keyword_count))
View(skills_df)
View(skills_df)
library(httr)
library(tidyverse)
library(jsonlite)
library(rvest)
library(reticulate)
##This chunk is concerned with getting the API function working
## Write a function to get any given page. After that I can just iterate while there are still jobs to find :)
get_adzuna_api <- function(api_id, api_key, page,
key_words = c("data", "analyst"), country = "ch") { #API ID and key are variables, so that they can remain secret
key_words <- stringr::str_c(key_words[1], "%20", key_words[2]) #Can only take two key words. All needed for most data professions
api_call_string <- stringr::str_c("http://api.adzuna.com/v1/api/jobs/",
country, "/search/", page, "?app_id=",
api_id, "&app_key=", api_key,
"&results_per_page=20&what=", key_words,
"&max_days_old=730&salary_include_unknown=1&content-type=application/json")
api_return <- GET(api_call_string)
api_return_text <- content(api_return, "text")
api_return_df <- jsonlite::fromJSON(api_return_text)
api_return_df$results
}
## Test the function
test <- get_adzuna_api(
readLines("/home/pete/Documents/adzuna_api.txt")[1],
readLines("/home/pete/Documents/adzuna_api.txt")[2], 10)
test$location$display_name
## This code block is about removing unwanted columns from the APi response and renaming other ones.
## Functionalizing for easier integration into the automated pipeline downstream
clean_adzuna_response <- function(adzuna_response) {
response_cleaned <- data.frame(
date_posted = adzuna_response$created,
job_title = adzuna_response$title,
company = adzuna_response$company$display_name,
location = adzuna_response$location$display_name,
website = adzuna_response$redirect_url
) %>%
filter(str_detect(website, "details")) #Adzuna links quite a few instances of external job boards
response_cleaned
}
### At first, this will be a mess of re-learning how to web scrape :P
parse_adzuna_data <- function (adzuna_http){
adzuna_html_response <- read_html(adzuna_http) #Get the website
adzuna_body <- adzuna_html_response %>% #Extract the body of the job ad
html_nodes(".adp-body") %>%
html_text()
adzuna_body
}
reticulate::repl_python()
