---
title: "The average Data Analyst job description"
author: "Pete Jacobsson"
from: markdown+emoji
format:
  html:
    theme: simplex
    css: styles.css
    sansfont: Corbel
    mainfont: Cambria
---

```{r}
library(tidyverse)
library(jsonlite)
library(rvest)
```

Hello everyone! The purpose of this little project is to work out what makes an average data analyst. If, like me, you've been around for a few years longer than the end of your undergraduate degree, you might also have the problem of how to figure out what to *cut* from your CV. Where this gets really interesting is the soft skills: should I push communication, or problem solving for example? Now, when applying for a job with a known job description this is not an issue - you just hack away things that match that description least. But what about the CV you take to conferences, networking events, or send to recruiters? If you're anything like me, you know that six pages is way too long, but what are the things to hack away? In principle, you can ask for a CV writing service advice. But that will cost you, and if you're a scrooge (like myself), paying is a deal breaker.

So I had a different idea - lets check what the average job description is and hack away things to match it. How to get the typical job description?

-   Get a lot of job descriptions (use API :grinning:)

-   Filter out the important stuff (use an LLM :smiley_cat:)

-   Analyze the results, work out what the descriptions require and hack your CV accordingly :scissors:


## Get the data I: API the websties
I will be looking for Data Analyst job posts from Switzerland. I'll use [Adzuna API](https://www.adzuna.ch/).

The core of the API request looks something like this:
```{r}
##This chunk is concerned with getting the API function working

## Write a function to get any given page. After that I can just iterate while there are still jobs to find :)

get_adzuna_api <- function(api_id, api_key, page, key_words) { #API ID and key are variables, so that they can remain secret
  
  key_words <- stringr::str_c(key_words[1], "%20", key_words[2]) #Can only take two key words. All needed for most data professions
  api_call_string <- stringr::str_c("http://api.adzuna.com/v1/api/jobs/ch/search/",  ##If I wanted to change country, it would be on this line
                                    page, "?app_id=", api_id, "&app_key=", api_key,
                                    "&results_per_page=20&what=", key_words,
                                    "&max_days_old=730&salary_include_unknown=1&content-type=application/json")
  api_return <- GET(api_call_string)
  api_return_text <- content(api_return, "text")
  api_return_df <- jsonlite::fromJSON(api_return_text)
  
  api_return_df$results
}
```


```{r}
## Test the function
test <- get_adzuna_api(
  readLines("/home/pete/Documents/adzuna_api.txt")[1], 
  readLines("/home/pete/Documents/adzuna_api.txt")[2], 10,
  c("data", "analyst"))


test$location$display_name

```


```{r}
## This code block is about removing unwanted columns from the APi response and renaming other ones.
## Functionalizing for easier integration into the automated pipeline downstream

clean_adzuna_response <- function(adzuna_response) {
  response_cleaned <- data.frame(
    date_posted = adzuna_response$created,
    job_title = adzuna_response$title,
    company = adzuna_response$company$display_name,
    location = adzuna_response$location$display_name,
    website = adzuna_response$redirect_url
  ) %>%
    filter(str_detect(website, "details")) #Adzuna links quite a few instances of external job boards
  
  
  response_cleaned
}
```

```{r}
##Test clean_adzuna_response
test2 <- clean_adzuna_response(test)
write_csv(test2, "test_returns.csv")
```




## Get the Data II: web-scrape
Because Adzuna API provides only snippets, we don't get the actual job descriptions to pass to CHat API for analysis. Now, what we do get, is links tio the Adzuna website. So what we can do to get the relevant data is to web scrape the list of pages provided and add the results to the outcomes table
```{r}
##Lets get a URL to visit
test2 <- read_csv("test_returns.csv")
test2$website[4]
```

```{r}
### At first, this will be a mess of re-learning how to web scrape :P


parse_adzuna_data <- function (adzuna_http){
  adzuna_html_response <- read_html(adzuna_http) #Get the website
  adzuna_body <- adzuna_html_response %>% #Extract the body of the job ad
    html_nodes(".adp-body") %>%
    html_text()
  
  adzuna_body
}

```
```{r}
##Test the function: run for the length of a whole DF
##Use for loop to ensure spacing between website hits
##Pass if test_vector contains 15 long extracted strings

### Commented out - don't want to run loops hitting people's websites by accident :)
# test_vector <- c()
# 
# for (i in 1:nrow(test2)) { #test2 contains the readings from the newest set of previous tests
#   adzuna_description <- parse_adzuna_data(test2$website[i])
#   test_vector <- c(test_vector, adzuna_description)
#   Sys.sleep(30)
# }

for (item in test_vector) {
  item_trunc <- str_trunc(item, 50)
  print(item_trunc)
}
```

We have a working webscrape. Now time to start getting the relevant skills.

## Get the data III Skills and experience extraction with Chat GPT

